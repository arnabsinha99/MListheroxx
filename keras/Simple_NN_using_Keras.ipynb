{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Simple_NN_using_Keras.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "pd9NfrGXFYSk",
        "colab_type": "code",
        "outputId": "1eee74f6-81f4-4219-d8f3-6cf0f784998f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "cell_type": "code",
      "source": [
        "import torch as tor\n",
        "import torch.nn as nn\n",
        "\n",
        "X = tor.tensor(([1,2],[4,5],[2,9]), dtype = tor.float)\n",
        "y = tor.tensor(([92],[89],[60]), dtype = tor.float)\n",
        "\n",
        "xPredicted = tor.tensor(([4,8]),dtype = tor.float)\n",
        "X_max, _ = tor.max(X,0)\n",
        "\n",
        "xPredicted_max, _ = tor.max(xPredicted, 0)\n",
        "\n",
        "X = tor.div(X, X_max)\n",
        "xPredicted = tor.div(xPredicted, xPredicted_max)\n",
        "y = y / 100 # max test score is 100\n",
        "\n",
        "''' now comes the asli code '''\n",
        "\n",
        "class neuralnetwork(nn.Module):\n",
        "  def __init__(self, ):\n",
        "    super(neuralnetwork, self).__init__()\n",
        "    \n",
        "    #pameters to be declared\n",
        "    \n",
        "    self.inputSize = 2\n",
        "    self.outputSize = 1\n",
        "    self.hiddenLayers = 3\n",
        "    \n",
        "    self.W1 = tor.randn(self.inputSize,self.hiddenLayers) #2 * 3 matrix for weights for i/p layers\n",
        "    self.W2 = tor.randn(self.hiddenLayers, self.outputSize) #1* 3 matrix for weights for o/p layers\n",
        "    \n",
        "  def forward(self, X):\n",
        "    \n",
        "    self.z = tor.matmul(X, self.W1)\n",
        "    self.z2 = self.sigmoid(self.z) # activation function\n",
        "    self.z3 = tor.matmul(self.z2, self.W2)\n",
        "    \n",
        "    out = tor.sigmoid(self.z3) #activation\n",
        "    \n",
        "    return out\n",
        " \n",
        "  def sigmoid(self, x):\n",
        "    \n",
        "    a = 1/(1+tor.exp(-x))\n",
        "    return a\n",
        "  \n",
        "  def sigmoid_d1(self,x):\n",
        "    \n",
        "    return x * (1-x)\n",
        "  \n",
        "  def backward(self,X,y,o):\n",
        "    \n",
        "    self.out_error = y-o\n",
        "    self.delta = self.sigmoid_d1(o) * self.out_error  #  h'(x) * (h(x) - y)\n",
        "    self.hidd_error = tor.matmul(self.delta,tor.t(self.W2))\n",
        "    self.delta2 = self.sigmoid_d1(self.hidd_error) * self.hidd_error\n",
        "    \n",
        "    self.W1 += tor.matmul(tor.t(X), self.delta2)\n",
        "    self.W2 += tor.matmul(tor.t(self.z2), self.delta)\n",
        "    \n",
        "  def train(self,X,y):\n",
        "    \n",
        "    o = self.forward(X)\n",
        "    self.backward(X,y,o)  \n",
        "    \n",
        "  def save_weights(self,model):\n",
        "    \n",
        "    tor.save(model,\"NN\")\n",
        "    \n",
        "  def predict(self):\n",
        "    \n",
        "    print(\"The predicted data based on trained weights are: \")\n",
        "    print (\"Input (scaled): \\n\" + str(xPredicted))\n",
        "    print (\"Output: \\n\" + str(self.forward(xPredicted).detach().item()*100))    \n",
        "\n",
        "\n",
        "NN = neuralnetwork()\n",
        "for i in range(1000):  # trains the NN 1,000 times\n",
        "    #print (\"#\" + str(i) + \" Loss: \" + str(tor.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
        "    NN.train(X, y)\n",
        "    \n",
        "NN.save_weights(NN)\n",
        "NN.predict()    \n",
        "  \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The predicted data based on trained weights are: \n",
            "Input (scaled): \n",
            "tensor([0.5000, 1.0000])\n",
            "Output: \n",
            "79.56417202949524\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:251: UserWarning: Couldn't retrieve source code for container of type neuralnetwork. It won't be checked for correctness upon loading.\n",
            "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xKfckda-WosO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "outputId": "dde96b0c-d656-4945-d4c5-6e05f22454bd"
      },
      "cell_type": "code",
      "source": [
        "import keras \n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "# importing IMDB Dataset\n",
        "\n",
        "from keras.datasets import imdb\n",
        "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# print(training_data.shape)  # (25000,)\n",
        "\n",
        "data = np.concatenate((training_data, testing_data), axis=0)\n",
        "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
        "\n",
        "print(\"Length os data is \" + str(len(data)))\n",
        "\n",
        "'''\n",
        "print(\"Categories:\", np.unique(targets))\n",
        "print(\"Number of unique words:\", len(np.unique(np.hstack(data))))\n",
        "\n",
        "a = 0\n",
        "print(data.shape)\n",
        "for i in data , a<3:\n",
        "  print (i)\n",
        "  a = a+1\n",
        "'''\n",
        "def vectorize(sequences, dimension = 10000):\n",
        " results = np.zeros((len(sequences), dimension))\n",
        " counter = 0\n",
        " for i, sequence in enumerate(sequences):\n",
        "  results[i, sequence] = 1\n",
        "  counter = counter + 1\n",
        " \n",
        " print(\"Counter is : \" + str(counter))\n",
        " return results\n",
        " \n",
        "data = vectorize(data)\n",
        "targets = np.array(targets).astype(\"float32\")\n",
        "\n",
        "test_x = data[:10000]\n",
        "test_y = targets[:10000]\n",
        "train_x = data[10000:]\n",
        "train_y = targets[10000:]\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "# Input Layer\n",
        "model.add(layers.Dense(50, activation = \"relu\", input_shape=(10000, )))\n",
        "\n",
        "# Hidden Layers\n",
        "model.add(layers.Dropout(0.3, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "model.add(layers.Dropout(0.2, noise_shape=None, seed=None))\n",
        "model.add(layers.Dense(50, activation = \"relu\"))\n",
        "\n",
        "# Output Layer\n",
        "model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
        "model.summary()\n",
        "\n",
        "# compiling the model\n",
        "model.compile( optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])\n",
        "results = model.fit( train_x, train_y, epochs= 2, batch_size = 500, validation_data = (test_x, test_y))\n",
        "print(\"Test-Accuracy:\", np.mean(results.history[\"val_acc\"]))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length os data is 50000\n",
            "Counter is : 50000\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 50)                500050    \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 1)                 51        \n",
            "=================================================================\n",
            "Total params: 505,201\n",
            "Trainable params: 505,201\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/2\n",
            "40000/40000 [==============================] - 7s 177us/step - loss: 0.4060 - acc: 0.8201 - val_loss: 0.2635 - val_acc: 0.8949\n",
            "Epoch 2/2\n",
            "40000/40000 [==============================] - 7s 164us/step - loss: 0.2121 - acc: 0.9200 - val_loss: 0.2601 - val_acc: 0.8937\n",
            "Test-Accuracy: 0.894300001859665\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}