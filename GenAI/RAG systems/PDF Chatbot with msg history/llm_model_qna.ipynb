{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import psycopg2\n",
    "from langchain_postgres import PGVector\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "from IPython.display import Markdown\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.chat_message_histories import SQLChatMessageHistory, ChatMessageHistory\n",
    "from langchain_community.vectorstores import FAISS, PGEmbedding\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing_extensions import Annotated, TypedDict\n",
    "from typing import Sequence\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define a dict representing the state of the application.\n",
    "# This state has the same input and output keys as `rag_chain`.\n",
    "class State(TypedDict):\n",
    "    input: str\n",
    "    chat_history: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    context: str\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatModelQnA():\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self._workflow = StateGraph(state_schema=State)\n",
    "        # Define the (single) node in the graph\n",
    "        self._workflow.add_edge(START, \"model\")\n",
    "        self._workflow.add_node(\"model\", self._call_model)\n",
    "\n",
    "        memory = MemorySaver()\n",
    "        # In the invocation process, _app can now handle config for personalized queries\n",
    "        self._app = self._workflow.compile(checkpointer=memory)\n",
    "        \n",
    "    def _call_model(self, state: State, config: dict = None):\n",
    "        # Use thread_id from config if provided\n",
    "        thread_id = config.get(\"configurable\", {}).get(\"thread_id\", None)\n",
    "\n",
    "        response = self._rag_chain.invoke(state)\n",
    "        return {\n",
    "            \"chat_history\": [\n",
    "                HumanMessage(state[\"input\"]),\n",
    "                AIMessage(response[\"answer\"]),\n",
    "            ],\n",
    "            \"context\": response[\"context\"],\n",
    "            \"answer\": response[\"answer\"],\n",
    "            \"thread_id\": thread_id  # Including thread_id in response if required\n",
    "        }\n",
    "    \n",
    "    def _ask_query(self, input_text: str, config: dict = None):\n",
    "        state = {\n",
    "            \"input\": input_text,\n",
    "            \"chat_history\": [],\n",
    "            \"context\": \"\",\n",
    "            \"answer\": \"\"\n",
    "        }\n",
    "        # Invoke _app with state and config for user-specific query handling\n",
    "        return self._app.invoke(state, config=config)\n",
    "\n",
    "    def _initialize_api(\n",
    "            self, \n",
    "            key_groq: str, \n",
    "            key_hf: str):\n",
    "        '''\n",
    "            Assigns Groq and HF API key to an object variable\n",
    "\n",
    "            Args:\n",
    "                self: reference to object\n",
    "                key: Groq API Key\n",
    "\n",
    "            Returns: None\n",
    "        '''\n",
    "        self._groq_api_key = os.getenv(key=key_groq)\n",
    "        self._hf_api_key = os.getenv(key=key_hf)\n",
    "        os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "        os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(key=\"LANGCHAIN_API_KEY\")\n",
    "        \n",
    "    def _initialize_model(\n",
    "            self, \n",
    "            model_name: str, \n",
    "            temperature: float,\n",
    "            embedding_model_name: str):\n",
    "        self._model_name = model_name\n",
    "        self._temperature = temperature\n",
    "        self._model = ChatGroq(model=self._model_name, groq_api_key=self._groq_api_key, temperature=self._temperature)\n",
    "        self._embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)\n",
    "    \n",
    "    def _initialize_retriever_chain(\n",
    "            self,\n",
    "            vector_store: PGVector\n",
    "    ):\n",
    "        self._vectorstore = vector_store\n",
    "        self._retriever = self._vectorstore.as_retriever()\n",
    "\n",
    "        # Contextualize question\n",
    "        contextualize_q_system_prompt = (\n",
    "            \"Given a chat history and the latest user question \"\n",
    "            \"which might reference context in the chat history, \"\n",
    "            \"formulate a standalone question which can be understood \"\n",
    "            \"without the chat history. Do NOT answer the question, \"\n",
    "            \"just reformulate it if needed and otherwise return it as is.\"\n",
    "        )\n",
    "\n",
    "        contextualize_q_prompt = ChatPromptTemplate(\n",
    "            [\n",
    "                (\"system\", contextualize_q_system_prompt),\n",
    "                MessagesPlaceholder(\"chat_history\"),\n",
    "                (\"human\", \"{input}\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self._history_aware_retriever = create_history_aware_retriever(self._model, self._retriever, contextualize_q_prompt)\n",
    "        \n",
    "        # Incorporate the history aware retriever into a question-answering chain.\n",
    "        self._system_prompt = (\n",
    "            \"You are an assistant for helping students for questions regarding academic policies or bylaws. You have been provided information from official sources.\"\n",
    "            \"Use ONLY the following pieces of retrieved context to answer. the question. If the answer can be quoted from the PDFs then do that.\"\n",
    "            \"If the question is not related to academic policies/bylaws then simply reply \\\"Sorry I cannot answer that question as of now\\\". If the question is relevant to academic policies/bylaws and you do not know the answer\" \n",
    "            \"then say that you DO NOT know. Please Keep the answer moderately concise.\"\n",
    "            \"\\n\\n\"\n",
    "            \"{context}\"\n",
    "        )\n",
    "\n",
    "        self._prompt = ChatPromptTemplate.from_messages(\n",
    "                    [\n",
    "                        (\"system\", self._system_prompt),\n",
    "                        MessagesPlaceholder(\"chat_history\"),\n",
    "                        (\"human\", \"{input}\"),\n",
    "                    ]\n",
    "                )\n",
    "        \n",
    "        self._question_answer_chain = create_stuff_documents_chain(self._model, self._prompt)\n",
    "        self._rag_chain = create_retrieval_chain(self._history_aware_retriever, self._question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "\n",
    "model_name = \"gemma2-9b-it\"\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "temperature = 0.1\n",
    "\n",
    "model_obj = ChatModelQnA()\n",
    "model_obj._initialize_api(\"GROQ_API_KEY\", \"HF_TOKEN\")\n",
    "model_obj._initialize_model(model_name=model_name, temperature=temperature, embedding_model_name=embedding_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connection = \"postgresql+psycopg://langchain:langchain@localhost:6024/langchain\"  # Uses psycopg3!\n",
    "connection=\"postgresql+psycopg://langchain:langchain321@54.147.167.63:5432/langchain\"\n",
    "collection_name = \"my_docs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = PGVector(\n",
    "    embeddings=model_obj._embeddings,\n",
    "    collection_name=collection_name,\n",
    "    connection=connection,\n",
    "    use_jsonb=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_store.similarity_search_with_relevance_scores(\"what is student audit?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_obj._initialize_retriever_chain(vector_store=vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I DO NOT know. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"user1\"}}\n",
    "result = model_obj._ask_query(\n",
    "    input_text=\"What are steps to get student audit for a course?\",\n",
    "    config=config\n",
    ")\n",
    "Markdown(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
