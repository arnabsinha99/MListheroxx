{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "<class 'dict'>\n",
      "{'source': 'speech.txt'}\n",
      "The world must be made safe for democracy. Its peace must be planted upon the tested foundations of \n"
     ]
    }
   ],
   "source": [
    "# Text Loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "# loader = TextLoader(\"second_hands_input.txt\")\n",
    "loader = TextLoader(\"speech.txt\")\n",
    "text = loader.load()\n",
    "print(len(text))\n",
    "print(type(text[0].metadata))\n",
    "print(text[0].metadata)\n",
    "print(text[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMP 8967 Summer 2024 Final Exam Question \\nPlease check the posted Excel Ô¨Åle, which shows each GA an'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PDF Loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"finalexam.pdf\")\n",
    "content = loader.load()\n",
    "content[0].page_content[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text splitter (RecursiveJson splitter case)\n",
    "Types of outputs\n",
    "<ul>\n",
    "<li>dict-type chunks</li>\n",
    "<li>document-type chunks</li>\n",
    "<li>string-type chunks</li> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from JSON\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = requests.get(\"https://api.smith.langchain.com/openapi.json\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_text_splitters.json.RecursiveJsonSplitter at 0x22c2b2e36b0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = RecursiveJsonSplitter(max_chunk_size=100)\n",
    "splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dict-type chunks after splitting\n",
    "json_chunks = splitter.split_json(json_data=json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}},\n",
       " {'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions']}}}},\n",
       " {'paths': {'/api/v1/sessions/{session_id}': {'get': {'summary': 'Read Tracer Session'}}}}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_chunks[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['/api/v1/sessions/{session_id}', '/api/v1/sessions', '/api/v1/sessions/{session_id}/metadata', '/api/v1/sessions/{session_id}/views', '/api/v1/sessions/{session_id}/views/{view_id}', '/api/v1/orgs', '/api/v1/orgs/current/setup', '/api/v1/orgs/current', '/api/v1/orgs/current/info', '/api/v1/orgs/current/billing', '/api/v1/orgs/current/dashboard', '/api/v1/orgs/current/payment-method', '/api/v1/orgs/current/business-info', '/api/v1/orgs/current/plan', '/api/v1/orgs/current/roles', '/api/v1/orgs/current/roles/{role_id}', '/api/v1/orgs/permissions', '/api/v1/orgs/pending', '/api/v1/orgs/current/members', '/api/v1/orgs/current/members/batch', '/api/v1/orgs/current/members/basic/batch', '/api/v1/orgs/current/members/{identity_id}/pending', '/api/v1/orgs/pending/{organization_id}', '/api/v1/orgs/pending/{organization_id}/claim', '/api/v1/orgs/current/members/{identity_id}', '/api/v1/orgs/members/basic', '/api/v1/orgs/ttl-settings', '/api/v1/orgs/current/sso-settings', '/api/v1/login', '/api/v1/api-key', '/api/v1/api-key/{api_key_id}', '/api/v1/api-key/current', '/api/v1/api-key/current/{pat_id}', '/api/v1/examples/{example_id}', '/api/v1/examples', '/api/v1/examples/bulk', '/api/v1/examples/upload/{dataset_id}', '/api/v1/datasets/{dataset_id}', '/api/v1/datasets', '/api/v1/datasets/upload', '/api/v1/datasets/upload-experiment', '/api/v1/datasets/{dataset_id}/versions', '/api/v1/datasets/{dataset_id}/versions/diff', '/api/v1/datasets/{dataset_id}/version', '/api/v1/datasets/{dataset_id}/tags', '/api/v1/datasets/{dataset_id}/openai', '/api/v1/datasets/{dataset_id}/openai_ft', '/api/v1/datasets/{dataset_id}/csv', '/api/v1/datasets/{dataset_id}/runs', '/api/v1/datasets/{dataset_id}/runs/delta', '/api/v1/datasets/{dataset_id}/share', '/api/v1/datasets/{dataset_id}/comparative', '/api/v1/datasets/comparative', '/api/v1/datasets/comparative/{comparative_experiment_id}', '/api/v1/datasets/clone', '/api/v1/datasets/{dataset_id}/splits', '/api/v1/datasets/{dataset_id}/index', '/api/v1/datasets/{dataset_id}/search', '/api/v1/datasets/{dataset_id}/generate', '/api/v1/runs/rules', '/api/v1/runs/rules/{rule_id}', '/api/v1/runs/rules/{rule_id}/logs', '/api/v1/runs/rules/{rule_id}/trigger', '/api/v1/runs/rules/trigger', '/api/v1/runs/{run_id}', '/api/v1/runs/{run_id}/share', '/api/v1/runs/query', '/api/v1/runs/generate-query', '/api/v1/runs/stats', '/api/v1/runs/monitor', '/api/v1/runs', '/api/v1/runs/batch', '/api/v1/runs/group', '/api/v1/runs/group/stats', '/api/v1/feedback/{feedback_id}', '/api/v1/feedback', '/api/v1/feedback/eager', '/api/v1/feedback/tokens', '/api/v1/feedback/tokens/{token}', '/api/v1/public/{share_token}/run', '/api/v1/public/{share_token}/run/{id}', '/api/v1/public/{share_token}/runs/query', '/api/v1/public/{share_token}/feedbacks', '/api/v1/public/{share_token}/datasets', '/api/v1/public/{share_token}/examples', '/api/v1/public/{share_token}/datasets/sessions', '/api/v1/public/{share_token}/examples/runs', '/api/v1/public/{share_token}/datasets/runs/delta', '/api/v1/public/{share_token}/datasets/runs/query', '/api/v1/public/{share_token}/datasets/runs/generate-query', '/api/v1/public/{share_token}/datasets/runs/stats', '/api/v1/public/{share_token}/datasets/runs/{run_id}', '/api/v1/public/{share_token}/datasets/feedback', '/api/v1/public/{share_token}/datasets/comparative', '/api/v1/annotation-queues', '/api/v1/annotation-queues/{queue_id}', '/api/v1/annotation-queues/{queue_id}/runs', '/api/v1/annotation-queues/{queue_id}/run/{index}', '/api/v1/annotation-queues/{run_id}/queues', '/api/v1/annotation-queues/{queue_id}/runs/{queue_run_id}', '/api/v1/annotation-queues/{queue_id}/total_size', '/api/v1/annotation-queues/{queue_id}/size', '/api/v1/annotation-queues/status/{annotation_queue_run_id}', '/api/v1/tenants', '/api/v1/tenants/pending', '/api/v1/tenants/pending/{id}', '/api/v1/tenants/pending/{tenant_id}/claim', '/api/v1/tenants/current/stats', '/api/v1/tenants/stats', '/api/v1/tenants/current/members', '/api/v1/tenants/current/roles', '/api/v1/tenants/current/shared', '/api/v1/tenants/current/members/{identity_id}', '/api/v1/tenants/current/members/{identity_id}/pending', '/api/v1/tenants/current/usage_limits', '/api/v1/tenants/current/secrets', '/api/v1/info', '/api/v1/feedback-configs', '/api/v1/model-price-map', '/api/v1/model-price-map/{id}', '/api/v1/usage-limits', '/api/v1/usage-limits/org', '/api/v1/usage-limits/{usage_limit_id}', '/api/v1/ttl-settings', '/api/v1/prompts/invoke_prompt', '/api/v1/workspaces', '/api/v1/workspaces/{workspace_id}', '/api/v1/workspaces/pending', '/api/v1/workspaces/pending/{id}', '/api/v1/workspaces/pending/{workspace_id}/claim', '/api/v1/workspaces/current/stats', '/api/v1/workspaces/current/members', '/api/v1/workspaces/current/members/batch', '/api/v1/workspaces/current/shared', '/api/v1/workspaces/current/members/{identity_id}', '/api/v1/workspaces/current/members/{identity_id}/pending', '/api/v1/workspaces/current/usage_limits', '/api/v1/workspaces/current/secrets', '/api/v1/workspaces/current/tag-keys', '/api/v1/workspaces/current/tag-keys/{tag_key_id}', '/api/v1/workspaces/current/tag-keys/{tag_key_id}/tag-values', '/api/v1/workspaces/current/tag-keys/{tag_key_id}/tag-values/{tag_value_id}', '/api/v1/workspaces/current/taggings', '/api/v1/workspaces/current/taggings/{tagging_id}', '/api/v1/workspaces/current/tags', '/api/v1/workspaces/current/tags/resource', '/api/v1/playground-settings', '/api/v1/playground-settings/{playground_settings_id}', '/api/v1/service-accounts', '/api/v1/service-accounts/{service_account_id}', '/api/v1/charts/section', '/api/v1/charts', '/api/v1/charts/preview', '/api/v1/charts/create', '/api/v1/charts/{chart_id}', '/api/v1/charts/section/{section_id}', '/api/v1/ok', '/api/v1/repos', '/api/v1/repos/{owner}/{repo}', '/api/v1/repos/{owner}/{repo}/fork', '/api/v1/repos/tags', '/api/v1/likes/{owner}/{repo}', '/api/v1/commits/{owner}/{repo}', '/api/v1/commits/{owner}/{repo}/{commit}', '/api/v1/settings', '/api/v1/settings/handle', '/api/v1/events', '/api/v1/comments/{owner}/{repo}', '/api/v1/comments/{owner}/{repo}/{parent_comment_id}', '/api/v1/comments/{owner}/{repo}/{parent_comment_id}/like'])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data['paths'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0: {'openapi': '3.1.0', 'info': {'title': 'LangSmith', 'version': '0.1.0'}}\n",
      "Chunk 1: {'paths': {'/api/v1/sessions/{session_id}': {'get': {'tags': ['tracer-sessions']}}}}\n",
      "Chunk 2: {'paths': {'/api/v1/sessions/{session_id}': {'get': {'summary': 'Read Tracer Session'}}}}\n",
      "Chunk 3: {'paths': {'/api/v1/sessions/{session_id}': {'get': {'description': 'Get a specific session.'}}}}\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    print(f'Chunk {i}: {json_chunks[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}}'),\n",
       " Document(page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"]}}}}'),\n",
       " Document(page_content='{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"summary\": \"Read Tracer Session\"}}}}')]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating documents after splitting\n",
    "docs = splitter.create_documents(texts=[json_data])\n",
    "docs[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"openapi\": \"3.1.0\", \"info\": {\"title\": \"LangSmith\", \"version\": \"0.1.0\"}}',\n",
       " '{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"tags\": [\"tracer-sessions\"]}}}}',\n",
       " '{\"paths\": {\"/api/v1/sessions/{session_id}\": {\"get\": {\"summary\": \"Read Tracer Session\"}}}}']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating string-based chunks after splitting\n",
    "text_chunks = splitter.split_text(json_data)\n",
    "text_chunks[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'dict'>\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "# difference in the chunk types\n",
    "print(type(text_chunks[0]))\n",
    "print(type(json_chunks[0]))\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "environ{'ALLUSERSPROFILE': 'C:\\\\ProgramData',\n",
       "        'APPDATA': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming',\n",
       "        'CHROME_CRASHPAD_PIPE_NAME': '\\\\\\\\.\\\\pipe\\\\crashpad_27064_SSQESBKMZRGGGFPX',\n",
       "        'COMMONPROGRAMFILES': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMMONPROGRAMFILES(X86)': 'C:\\\\Program Files (x86)\\\\Common Files',\n",
       "        'COMMONPROGRAMW6432': 'C:\\\\Program Files\\\\Common Files',\n",
       "        'COMPUTERNAME': 'ACER',\n",
       "        'COMSPEC': 'C:\\\\Windows\\\\system32\\\\cmd.exe',\n",
       "        'CONDA_ALLOW_SOFTLINKS': 'false',\n",
       "        'CONDA_DEFAULT_ENV': 'base',\n",
       "        'CONDA_EXE': 'C:\\\\Users\\\\Admin\\\\anaconda3\\\\Scripts\\\\conda.exe',\n",
       "        'CONDA_PREFIX': 'C:\\\\Users\\\\Admin\\\\anaconda3',\n",
       "        'CONDA_PROMPT_MODIFIER': '(base) ',\n",
       "        'CONDA_PYTHON_EXE': 'C:\\\\Users\\\\Admin\\\\anaconda3\\\\python.exe',\n",
       "        'CONDA_ROOT': 'C:\\\\Users\\\\Admin\\\\anaconda3',\n",
       "        'CONDA_SHLVL': '1',\n",
       "        'DRIVERDATA': 'C:\\\\Windows\\\\System32\\\\Drivers\\\\DriverData',\n",
       "        'EFC_5384': '1',\n",
       "        'ELECTRON_RUN_AS_NODE': '1',\n",
       "        'HOMEDRIVE': 'C:',\n",
       "        'HOMEPATH': '\\\\Users\\\\Admin',\n",
       "        'JPY_INTERRUPT_EVENT': '2152',\n",
       "        'LOCALAPPDATA': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local',\n",
       "        'LOGONSERVER': '\\\\\\\\ACER',\n",
       "        'NUMBER_OF_PROCESSORS': '8',\n",
       "        'ONEDRIVE': 'C:\\\\Users\\\\Admin\\\\OneDrive',\n",
       "        'ONEDRIVECONSUMER': 'C:\\\\Users\\\\Admin\\\\OneDrive',\n",
       "        'ORIGINAL_XDG_CURRENT_DESKTOP': 'undefined',\n",
       "        'OS': 'Windows_NT',\n",
       "        'PATH': 'c:\\\\Users\\\\Admin\\\\anaconda3;C:\\\\Users\\\\Admin\\\\anaconda3;C:\\\\Users\\\\Admin\\\\anaconda3\\\\Library\\\\mingw-w64\\\\bin;C:\\\\Users\\\\Admin\\\\anaconda3\\\\Library\\\\usr\\\\bin;C:\\\\Users\\\\Admin\\\\anaconda3\\\\Library\\\\bin;C:\\\\Users\\\\Admin\\\\anaconda3\\\\Scripts;C:\\\\Users\\\\Admin\\\\anaconda3\\\\bin;C:\\\\Users\\\\Admin\\\\anaconda3\\\\condabin;C:\\\\Windows\\\\system32;C:\\\\Windows;C:\\\\Windows\\\\System32\\\\Wbem;C:\\\\Windows\\\\System32\\\\WindowsPowerShell\\\\v1.0;C:\\\\Windows\\\\System32\\\\OpenSSH;C:\\\\Program Files\\\\Git\\\\cmd;C:\\\\MinGW\\\\bin;.;C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVIDIA NvDLISR;C:\\\\Program Files (x86)\\\\NVIDIA Corporation\\\\PhysX\\\\Common;C:\\\\Program Files\\\\nodejs;C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38\\\\Scripts;C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python38;C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Microsoft\\\\WindowsApps;C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code\\\\bin;C:\\\\Program Files\\\\JetBrains\\\\PyCharm 2024.1.1\\\\bin;.;C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\GitHubDesktop\\\\bin;C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\npm',\n",
       "        'PATHEXT': '.COM;.EXE;.BAT;.CMD;.VBS;.VBE;.JS;.JSE;.WSF;.WSH;.MSC',\n",
       "        'PROCESSOR_ARCHITECTURE': 'AMD64',\n",
       "        'PROCESSOR_IDENTIFIER': 'Intel64 Family 6 Model 158 Stepping 10, GenuineIntel',\n",
       "        'PROCESSOR_LEVEL': '6',\n",
       "        'PROCESSOR_REVISION': '9e0a',\n",
       "        'PROGRAMDATA': 'C:\\\\ProgramData',\n",
       "        'PROGRAMFILES': 'C:\\\\Program Files',\n",
       "        'PROGRAMFILES(X86)': 'C:\\\\Program Files (x86)',\n",
       "        'PROGRAMW6432': 'C:\\\\Program Files',\n",
       "        'PROMPT': '(base) $P$G',\n",
       "        'PSMODULEPATH': 'C:\\\\Program Files\\\\WindowsPowerShell\\\\Modules;C:\\\\Windows\\\\system32\\\\WindowsPowerShell\\\\v1.0\\\\Modules',\n",
       "        'PUBLIC': 'C:\\\\Users\\\\Public',\n",
       "        'PYCHARM': 'C:\\\\Program Files\\\\JetBrains\\\\PyCharm 2024.1.1\\\\bin;',\n",
       "        'PYDEVD_IPYTHON_COMPATIBLE_DEBUGGING': '1',\n",
       "        'PYTHONIOENCODING': 'utf-8',\n",
       "        'PYTHONUNBUFFERED': '1',\n",
       "        'PYTHONUTF8': '1',\n",
       "        'PYTHON_FROZEN_MODULES': 'on',\n",
       "        'SESSIONNAME': 'Console',\n",
       "        'SSL_CERT_FILE': 'C:\\\\Users\\\\Admin\\\\anaconda3\\\\Library\\\\ssl\\\\cacert.pem',\n",
       "        'SYSTEMDRIVE': 'C:',\n",
       "        'SYSTEMROOT': 'C:\\\\Windows',\n",
       "        'TEMP': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Temp',\n",
       "        'TMP': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Temp',\n",
       "        'USERDOMAIN': 'Acer',\n",
       "        'USERDOMAIN_ROAMINGPROFILE': 'Acer',\n",
       "        'USERNAME': 'Admin',\n",
       "        'USERPROFILE': 'C:\\\\Users\\\\Admin',\n",
       "        'VBOX_MSI_INSTALL_PATH': 'C:\\\\Program Files\\\\Oracle\\\\VirtualBox\\\\',\n",
       "        'VSCODE_AMD_ENTRYPOINT': 'vs/workbench/api/node/extensionHostProcess',\n",
       "        'VSCODE_CODE_CACHE_PATH': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Roaming\\\\Code\\\\CachedData\\\\fee1edb8d6d72a0ddff41e5f71a671c23ed924b9',\n",
       "        'VSCODE_CRASH_REPORTER_PROCESS_TYPE': 'extensionHost',\n",
       "        'VSCODE_CWD': 'C:\\\\Users\\\\Admin\\\\AppData\\\\Local\\\\Programs\\\\Microsoft VS Code',\n",
       "        'VSCODE_HANDLES_UNCAUGHT_ERRORS': 'true',\n",
       "        'VSCODE_IPC_HOOK': '\\\\\\\\.\\\\pipe\\\\6dad4cc6-1.92.2-main-sock',\n",
       "        'VSCODE_L10N_BUNDLE_LOCATION': '',\n",
       "        'VSCODE_NLS_CONFIG': '{\"userLocale\":\"en-us\",\"osLocale\":\"en-in\",\"resolvedLanguage\":\"en\",\"defaultMessagesFile\":\"C:\\\\\\\\Users\\\\\\\\Admin\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Microsoft VS Code\\\\\\\\resources\\\\\\\\app\\\\\\\\out\\\\\\\\nls.messages.json\",\"locale\":\"en\",\"availableLanguages\":{}}',\n",
       "        'VSCODE_PID': '27064',\n",
       "        'WINDIR': 'C:\\\\Windows',\n",
       "        'ZES_ENABLE_SYSMAN': '1',\n",
       "        '_CONDA_OLD_CHCP': '437',\n",
       "        '__CONDA_OPENSLL_CERT_FILE_SET': '\"1\"',\n",
       "        'PYDEVD_USE_FRAME_EVAL': 'NO',\n",
       "        'TERM': 'xterm-color',\n",
       "        'CLICOLOR': '1',\n",
       "        'FORCE_COLOR': '1',\n",
       "        'CLICOLOR_FORCE': '1',\n",
       "        'PAGER': 'cat',\n",
       "        'GIT_PAGER': 'cat',\n",
       "        'MPLBACKEND': 'module://matplotlib_inline.backend_inline',\n",
       "        'OPENAI_API_KEY': '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',\n",
       "        'LANGCHAIN_API_KEY': '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',\n",
       "        'LANGCHAIN_PROJECT': 'GenAIAppWithOpenAI',\n",
       "        'HF_TOKEN': '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'}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIMPLE CHATBOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import langchain_core\n",
    "import langchain_community\n",
    "import langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"HF_TOKEN\"] = os.getenv(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/v0.2/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non streaming output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HuggingFaceEndpoint: https://python.langchain.com/v0.2/api_reference/huggingface/llms/langchain_huggingface.llms.huggingface_endpoint.HuggingFaceEndpoint.html\n",
    "\n",
    "ChatHuggingFace: https://python.langchain.com/v0.2/api_reference/huggingface/chat_models/langchain_huggingface.chat_models.huggingface.ChatHuggingFace.html#langchain_huggingface.chat_models.huggingface.ChatHuggingFace.invoke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "llm_model = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatHuggingFace(llm=HuggingFaceEndpoint(repo_id='HuggingFaceH4/zephyr-7b-beta', temperature=0.01, model='HuggingFaceH4/zephyr-7b-beta', client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, async_client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, task='text-generation'), tokenizer=LlamaTokenizerFast(name_or_path='HuggingFaceH4/zephyr-7b-beta', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, model_id='HuggingFaceH4/zephyr-7b-beta')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_model = ChatHuggingFace(llm=llm_model)\n",
    "chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Deep Learning is a subset of Machine Learning that uses artificial neural networks with multiple layers to learn and make predictions or decisions directly from raw data. These neural networks, modelled after the structure of the human brain, can learn and identify complex patterns and features in large data sets that would be difficult or impossible for traditional statistical and machine learning techniques to detect. Deep Learning algorithms have shown impressive results in various applications such as image and speech recognition, natural language processing, and autonomous driving.' response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=97, prompt_tokens=29, total_tokens=126), 'model': '', 'finish_reason': 'eos_token'} id='run-805bde54-e22a-4106-9763-4dcf4889ce3a-0'\n"
     ]
    }
   ],
   "source": [
    "print(chat_model.invoke(\"WHat is deep learning?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm=HuggingFaceEndpoint(callbacks=[<langchain_core.callbacks.streaming_stdout.StreamingStdOutCallbackHandler object at 0x0000022C464124E0>], repo_id='HuggingFaceH4/zephyr-7b-beta', temperature=0.01, streaming=True, model='HuggingFaceH4/zephyr-7b-beta', client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, async_client=<InferenceClient(model='HuggingFaceH4/zephyr-7b-beta', timeout=120)>, task='text-generation') tokenizer=LlamaTokenizerFast(name_or_path='HuggingFaceH4/zephyr-7b-beta', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='left', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "} model_id='HuggingFaceH4/zephyr-7b-beta'\n",
      "content='I\\'m sorry, but I don\\'t have a physical body or a name to go with it. I\\'m a virtual assistant powered by AI and my main objective is to provide helpful and accurate answers to questions asked by my users. My name is not relevant, and I prefer to be referred to by my function or role, like \"customer support assistant\" or \"online tutor.\" Thank you for your understanding. If you have any other questions, feel free to ask!' response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=99, prompt_tokens=29, total_tokens=128), 'model': '', 'finish_reason': 'eos_token'} id='run-d723d1df-1059-4c93-ac9f-b681bd26635d-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "callbacks = [StreamingStdOutCallbackHandler()]\n",
    "llm_model_stream = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    temperature=0.01,\n",
    "    callbacks=callbacks,\n",
    "    streaming=True,\n",
    ")\n",
    "chat_model_stream = ChatHuggingFace(llm=llm_model_stream)\n",
    "print(chat_model_stream)\n",
    "print(chat_model_stream.invoke(\"WHat is your name?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is Bob.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='What is your name?')])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\", \"Hello, how are you doing?\"),\n",
    "    (\"ai\", \"I'm doing well, thanks!\"),\n",
    "    (\"human\", \"{user_input}\"),\n",
    "])\n",
    "# filling the template initially itself\n",
    "prompt = template.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ")\n",
    "prompt # ChatPromptValue type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob, how may I assist you today?', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=13, prompt_tokens=95, total_tokens=108), 'model': '', 'finish_reason': 'eos_token'}, id='run-b3bc1bca-0370-46b3-aa05-c5ce70c8e450-0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prompt|chat_model # gave error\n",
    "chat_model.invoke(prompt) # pass ChatPromptValue into invoke of chat_model (ChatHuggingFace())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = template|chat_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is Bob, how may I assist you today?', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=13, prompt_tokens=95, total_tokens=108), 'model': '', 'finish_reason': 'eos_token'}, id='run-7d426588-8a7d-427f-b81d-3f98e7525f83-0')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name?\"\n",
    "    }\n",
    ") # pass dict into invoke of chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So basically, you create a chain with the prompt as a ChatPromptTemplate() type and then you call chain.invoke() with a dict as input to the prompt. Invoke basically passes the dict to prompt and the pipe reaction continues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My name is Bob. Apples are round or oval-shaped fruit that grow on trees called apple trees. They are usually red, green, or yellow in color, and have a fleshy texture and a sweet or tart taste, depending on the variety. Apples are a popular fruit that is commonly eaten raw, juiced, or cooked in various dishes like pies, sauces, and salads. They are also rich in fiber, vitamin C, and other nutri"
     ]
    }
   ],
   "source": [
    "for s in chain.stream(\n",
    "    {\n",
    "        \"name\": \"Bob\",\n",
    "        \"user_input\": \"What is your name? Explain about apples.\"\n",
    "    }\n",
    "):\n",
    "    print(s.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 (Using SystemMessage, HumanMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"System: You are a helpful AI bot. Your name is {name}.\\nHuman: Hello, how are you doing?\\nAI: I'm doing well, thanks!\\nHuman: {user_input}\""
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT WORKING\n",
    "template = ChatPromptTemplate([\n",
    "    SystemMessage(content=\"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    HumanMessage(content=\"Hello, how are you doing?\"),\n",
    "    AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "    HumanMessage(content=\"{user_input}\"),\n",
    "])\n",
    "# filling the template initially itself\n",
    "prompt = template.format(name=\"Bob\",user_input=\"What is your name?\")\n",
    "prompt # ChatPromptValue type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful AI bot. Your name is {name}.'), HumanMessage(content='Hello, how are you doing?'), AIMessage(content=\"I'm doing well, thanks!\"), HumanMessage(content='{user_input}')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NOT WORKING\n",
    "template = ChatPromptTemplate([\n",
    "    SystemMessage(content=\"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    HumanMessage(content=\"Hello, how are you doing?\"),\n",
    "    AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "    HumanMessage(content=\"{user_input}\"),\n",
    "])\n",
    "# filling the template initially itself\n",
    "prompt = template.invoke({\n",
    "    \"name\":\"Bob\",\n",
    "    \"user_input\":\"What is your name?\"\n",
    "    }\n",
    ")\n",
    "prompt # ChatPromptValue type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOT WORKING\n",
    "template = ChatPromptTemplate([\n",
    "    SystemMessage(content=\"You are a helpful AI bot. Your name is Aaron.\"),\n",
    "    HumanMessage(content=\"Hello, how are you doing?\"),\n",
    "    AIMessage(content=\"I'm doing well, thanks!\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tried to use SystemMessage(), HumanMessage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Magnus est le meilleur.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORKING\n",
    "message = [\n",
    "    SystemMessage(content=\"You are a translator. Translate the given sentence from English to French. Only return the translated message.\"),\n",
    "    HumanMessage(content=\"Magnus is the best.\")\n",
    "]\n",
    "chat_model.invoke(message).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['input1', 'name'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['name'], template='You are a helpful AI bot. Your name is {name}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input1'], template='{input1}'))])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORKING\n",
    "template = ChatPromptTemplate([\n",
    "    (\"system\",\"You are a helpful AI bot. Your name is {name}.\"),\n",
    "    (\"human\",\"{input1}\"),\n",
    "])\n",
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='My name is BEN, short for Bilingual Exchangeable Network. As for your second question, the distance between Navi Mumbai and Delhi is approximately 1,333 kilometers (827 miles) by road via National Highway 8. The actual travel time would vary depending on various factors such as traffic conditions and the mode of transportation used.', response_metadata={'token_usage': ChatCompletionOutputUsage(completion_tokens=77, prompt_tokens=65, total_tokens=142), 'model': '', 'finish_reason': 'eos_token'}, id='run-2326b623-c57d-4642-823e-e6eb18222c17-0')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template|chat_model\n",
    "chain.invoke({\n",
    "    \"name\": \"BEN\",\n",
    "    \"input1\": \"What is your name? How far is Navi Mumbai from Delhi?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parsers example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating chain and then passing the parameters to template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is BEN, but you can also call me an AI bot or virtual assistant. BEN is just a friendly name that I use to interact with users like you. Is there anything else I can help you with today? Just let me know!'"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "op_parser = StrOutputParser()\n",
    "chain = template|chat_model|op_parser\n",
    "\n",
    "chain.invoke({\n",
    "    \"name\": \"BEN\",\n",
    "    \"input1\": \"What is your name?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm not capable of having personal preferences or liking anything. My primary function is to assist and provide accurate information based on available data. So, I do not have any opinions on ronaldo or messi, but I can provide information and stats on their performances if needed.\""
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "    \"name\": \"BROOO\",\n",
    "    \"input1\": \"Please be specific. Do not give diplomatic answers. Do you like Ronaldo or Messi?\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this model is a dumbo.. obvio messi is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating chain till chat_model and then calling invoke() on parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question \"Which came first, the chicken or the egg?\" is a classic philosophical riddle that attempts to determine the chronological order of two seemingly interconnected events.\\n\\nThe answer to this riddle is inconclusive as it assumes a completely unobserved scenario, and there is no definitive answer. However, in the natural world, scientists believe that the first birds that existed were most likely born from pre-existing bird species, meaning an egg already existed before the first \"mod'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "op_parser = StrOutputParser()\n",
    "chain = template|chat_model\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\n",
    "    \"name\": \"BROOO\",\n",
    "    \"input1\": \"CHicken or egg? Which came first?\"\n",
    "}\n",
    ")\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
