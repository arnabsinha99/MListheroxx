# MListheroxx

This repo is meant for a self progress record. I will keep updating it with the posts, topics and websites I refer to in order to learn new concepts, codes and various other things in the field of Data Science, ML and AI.

### Useful links that I find while studying Machine Learning and AI. 

* [NIN(Network In Network](http://teleported.in/posts/network-in-network/) 
   
   Had an explanation of MLPConv and Global Average Pooling in it. 
  
* [Implementation of Dropout and it explanation](https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/   )

   This explains the concept of Dropout Layers, a recent finding, that seems to increase the general performance of Deep Learning models.
   The basic idea is to drop random nodes in a particular layer so as to generalize the performance of each parameter and not create a bias 
   performance. The following link has the code as well.

* [Convolution NN for Visual Recognition CS231n](http://cs231n.github.io/convolutional-networks/#pool)

  This has a complete explanation with Math about the different types of CNN Layers, CNN Architecture and additional references. 

* [Softmax Classifier](https://www.pyimagesearch.com/2016/09/12/softmax-classifiers-explained/)

   This explains the difference between Softmax Classifier and the Cross entropy function. Cross entropy function uses the softmax
   expression to evaluate the loss function value. The following link also has the implementation of Softmax classifier. Keras uses a direct
   function for setting the optimizer. 
   
* [Comprehensive Guide to CNN](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)

   This very neatly and briefly explains about the various layers of a classical CNN and how things work out in one. You might well get a
   beautiful understanding of many of the basic terms in CNN.

*  [Why is bias necessary in ANN](https://stackoverflow.com/questions/7175099/why-the-bias-is-necessary-in-ann-should-we-have-separate-bias-for-each-layer)
   
   This has a pretty good answer as to why we need to include bias in ANN.

* [Everything you need to know about NN](https://medium.com/ravenprotocol/everything-you-need-to-know-about-neural-networks-6fcc7a15cb4)

   It gives a pretty good explanation of most of the topics that one would encounter in the field of NN. 

* [A brief view of machine learning pipeline in python](https://medium.com/@yanhann10/a-brief-view-of-machine-learning-pipeline-in-python-5f50b941fca8) 

   I learnt a new concept of pipelining in Python where we can create a structure to implement various algorithms back to back in a
   pipeline form. 

* [Primer for learning Google Colab](https://medium.com/dair-ai/primer-for-learning-google-colab-bb4cabca5dd6)

   This is a very good site to learn Google Colab! Do visit it!   

* [Simple NN from Scratch](https://medium.com/dair-ai/a-simple-neural-network-from-scratch-with-pytorch-and-google-colab-c7f3830618e0)

   Simple implementation of NN with Pytorch.    

* [A detailed look into the top three activation functions](https://link.medium.com/cNOrhEkJnU)

   A good lookthrough at the reasons behind choosing one of the three top 3 activation functions. 

* [Introduction to Pytorch](https://medium.com/deeplearningbrasilia/deep-learning/-introduction-to-pytorch-5bd39421c84)

* [How to build a Neural Network with Keras](https://towardsdatascience.com/how-to-build-a-neural-network-with-keras-e8faa33d0ae4)

   Creating a NN using IMDB dataset in Keras.

* [NLP Classifiers with Transfer Learning and Weak Supervision](https://towardsdatascience.com/a-technique-for-building-nlp-classifiers-efficiently-with-transfer-learning-and-weak-supervision-a8e2f21ca9c8?source=email-anon_d3c65036a267--publication.newsletter)

* [CAN BUS](https://www.csselectronics.com/screen/page/simple-intro-to-can-bus/language/en)

   This is an excellent intro to CANBUS Network. If one wants to work on data from vehicles in the field of Data Science, you can very
   easily understand the mechanics from CSSElectronic's intro to CANBUS Network, OBD2 and the tools to extract the data. 

* [Math behind SVM - One of the best explanations](https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/)

   You might benefit a lot from this blog about the Math behind SVM. It is one of the finest I have come across.
   
* [Machine Learning with NLP and Text Recognition](https://levelup.gitconnected.com/machine-learning-with-python-nlp-and-text-recognition-94444d55b0ef)

* [Pipelining in Python](https://medium.com/@yanhann10/a-brief-view-of-machine-learning-pipeline-in-python-5f50b941fca8)

   I had never thought about this but pipelining does exist in Python language. Much needed to implement various models in ML and AI. Do
   have a read at this!
   
* [Importing datasets in Google Colab](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92)

   If you are using Google Colab and you find difficulty in importing datasets, follow this link
   
* [Learning AI](https://skymind.ai/wiki/)

   A great link if you want to get a general most of the concepts related to AI.

* [Master Link](https://docs.google.com/spreadsheets/d/1brGCIAQUl1LKIMQSaYGnLGXmJZX9jt4PrgR8n-MfPDg/edit#gid=0)

   Master link to which framework for which domain!

* [Develop first xgboost model python scikit learn](https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/)

   Took syntax help from here for animal prediction challenge. Do refer for reference!

* [Complete Guide to Parameter Tuning in Gradient Boosting (GBM) in Python](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/) 

  [Complete Guide to Parameter Tuning in XGBoost (with codes in Python)](https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/)
  
* [Parallel Gradient Boosting Decision Trees](http://zhanpengfang.github.io/418home.html)
  
  The above three links are references to XGBoost and Gradient Boosting. It will help you to understand the working behind one of the most
  influential algorithms in today's Data Science world. 
  
* [Reading a csv file with different parameters](https://chrisalbon.com/python/data_wrangling/pandas_dataframe_importing_csv/)
  
  It is a good article about reading a .csv file into your code by utilizing various parameters in the .read.csv() function. 
  
* [Gradient Boosting - The Math](https://explained.ai/gradient-boosting/L2-loss.html#sec:2.3)
  
  Excellent article to understand the intuition and quite a bit Math behind Gradient Boosting.
  
* [Semantic Segmentation in Image Processing](https://medium.com/beyondminds/a-simple-guide-to-semantic-segmentation-effcf83e7e54)

  A good article to understand the approach of classical as well as DL methods of semantic segmentation of images. Might have to refer the
  internet to understand underlying sub-topics within this article. Informative for sure!
  
* [ROC curves and analysis using them](https://acutecaretesting.org/en/articles/roc-curves-what-are-they-and-how-are-they-used)

  ROC curves are a pretty good way of checking as to how accurate the predictions have been made by your model. The Area under the ROC curve
  determines how well the model has performed. Higher the area, higher the precision. To read more, follow the link!

  [Terms relating to a confusion matrix](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)
  
  No other better source to understand a confusion matrix completely. Please do read this!
  
* [Attention in RNN](https://medium.com/datadriveninvestor/attention-in-rnns-321fbcd64f05)

  Explains about the topic of Attention in RNN. It is used to ease the task of context learning by focusing on certain parts of the data.
  Do have a read!

* [Level-Set Method in Computer Vision](https://wiseodd.github.io/techblog/2016/11/05/levelset-method/)

  A method to align the orientation of images using CV and model the movement of curves which cannot be done using DNN easily. Do have a
  look at the math and the Python implementation. 

* [Time series prediction using LSTM](https://blog.statsbot.co/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f)

* [Trends levels and seasonality in Time series](https://machinelearningmastery.com/decompose-time-series-data-trend-seasonality/)

   A good article explaining about the three terms level, trend and seasonality that are very important to understand Time Series.

* [Hough Transform for Edge Linking](http://aishack.in/tutorials/hough-transform-basics/)

   Refer the two links given at the bottom of this page, i.e., Basics and Normal form. A neat tutorial of Hough transform required for
   edge linking on Edge detected images.

* [Otsu Thresholding](http://www.labbookpages.co.uk/software/imgProc/otsuThreshold.html)

   Optimal thresholding technique in Image Processing. *This is not in ML but I found this worthy enough to be mentioned as it is a major part of pre-processing*
